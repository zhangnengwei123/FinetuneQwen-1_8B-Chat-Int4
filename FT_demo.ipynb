{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c797a8c4-3bb0-4ee1-b0c3-97bb5ec5d568",
   "metadata": {},
   "source": [
    "# Running Qwen, the base language model, is also simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8c24dd-e082-4e3e-982d-a7146a485032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 03:29:44,240 - modelscope - INFO - PyTorch version 2.3.1+cu118 Found.\n",
      "2024-06-22 03:29:44,242 - modelscope - INFO - Loading ast index from /home/ubuntu/.cache/modelscope/ast_indexer\n",
      "2024-06-22 03:29:44,293 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 1ded9c52ecd449140f691a5f467acd25 and a total number of 980 components indexed\n",
      "/home/ubuntu/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-06-22 03:29:47,907 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Some weights of the model checkpoint at /home/ubuntu/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.8.mlp.w2.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.9.mlp.w1.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Downloading model checkpoint to a local dir model_dir\n",
    "# model_dir = snapshot_download('qwen/Qwen-7B')\n",
    "# model_dir = snapshot_download('qwen/Qwen-7B-Chat')\n",
    "# model_dir = snapshot_download('qwen/Qwen-14B')\n",
    "model_dir = snapshot_download('qwen/Qwen-1_8B-Chat-Int4')\n",
    "\n",
    "# Loading local checkpoints\n",
    "# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411941f9-b9da-4065-a705-dc38d0098416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"qwen/Qwen-1_8B-Chat-Int4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c635ecf-111c-438d-bb2c-5d2e15f89121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 06:24:05,359 - modelscope - INFO - PyTorch version 2.3.1+cu118 Found.\n",
      "2024-06-22 06:24:05,361 - modelscope - INFO - Loading ast index from /home/ubuntu/.cache/modelscope/ast_indexer\n",
      "2024-06-22 06:24:05,849 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 1ded9c52ecd449140f691a5f467acd25 and a total number of 980 components indexed\n",
      "/home/ubuntu/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-06-22 06:24:09,229 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2024-06-22 06:24:10,570 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Some weights of the model checkpoint at /home/ubuntu/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.3.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-06-22 06:24:27,537 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！很高兴见到你。有什么我可以帮助你的吗？\n",
      "浙江省的省会是杭州。\n",
      "杭州有许多有趣的地方，其中最著名的可能是西湖和灵隐寺。西湖是中国最著名的风景名胜区之一，拥有许多美丽的景点，如雷峰塔、三潭印月等。灵隐寺是中国最早的佛教寺庙之一，它有着悠久的历史，并且保存了许多古代文物。此外，杭州还有许多其他有趣的景点，例如江湾、楼外楼、断桥残雪等。\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "# Model names: \"qwen/Qwen-7B-Chat\", \"qwen/Qwen-14B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name, trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"浙江的省会在哪里？\", history=history) \n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"它有什么好玩的景点\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb4e3c1-9f49-4414-b3dc-0cdde13ac50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_model_id = \"/home/ubuntu/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d7507-aba8-4396-b8cb-7e135284ffa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d873e2-11e0-4ed2-850a-1b0823e19314",
   "metadata": {},
   "source": [
    "# Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c5cacd-4430-4c4b-a871-4e502a788f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分析问题中出现过的城市和日期\n",
      "\n",
      "交互格式如下：\n",
      "\n",
      "问题：用户的原始问题，里面可能包含了城市和日期信息\n",
      "回答：提炼问题中出现的城市名称和具体日期，按照格式整理返回，\n",
      "    城市：值是城市的名字\n",
      "    日期：值是具体日期\n",
      "\n",
      "现在开始。\n",
      "\n",
      "问题：1月2号上海天气预报\n",
      "回答：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = \"1月2号上海天气预报\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "分析问题中出现过的城市和日期\n",
    "\n",
    "交互格式如下：\n",
    "\n",
    "问题：用户的原始问题，里面可能包含了城市和日期信息\n",
    "回答：提炼问题中出现的城市名称和具体日期，按照格式整理返回，\n",
    "    城市：值是城市的名字\n",
    "    日期：值是具体日期\n",
    "\n",
    "现在开始。\n",
    "\n",
    "问题：%s\n",
    "回答：\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt_template%(Q,)\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828d217a-4fa3-4b14-99f3-27ebad0a0572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题：1月2号的上海天气预报\n",
      "回答：01/02/2023 14:05:00 阴转多云 \n",
      "城市：上海  \n",
      "日期：2023-01-02\n"
     ]
    }
   ],
   "source": [
    "resp,hist = model.chat(tokenizer,prompt,history=None)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d519ff-b3a0-4488-8e00-9c5d6c508cd9",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511eb4b-d2e4-4d46-8979-0863148e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "# pip install \"peft<0.8.0\" deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0acbe-67da-483d-bb71-dfc3fbe0f248",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63c44aa-54f5-43f2-a3cc-c29028f6c891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'identity_0',\n",
       "  'conversations': [{'from': 'user', 'value': '你好'},\n",
       "   {'from': 'assistant', 'value': '我是一个语言模型，我叫通义千问。'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你好\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"我是一个语言模型，我叫通义千问。\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e8448ca-b74c-49cb-a02b-a2a67376d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量: 1000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "train_data = []\n",
    "\n",
    "Q_list = [\n",
    "    ('{city}{year}年{month}月{day}日的天气','%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气','%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气','%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气','%m-%d'),\n",
    "\n",
    "    ('{year}年{month}月{day}日{city}的天气','%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气','%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气','%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气','%m-%d'),\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "city_list = ['上海','北京','南京','苏州','杭州','南通','盐城','南昌','西安','成都','东京','台北']\n",
    "\n",
    "for i in range(1000):\n",
    "    Q = Q_list[random.randint(0,len(Q_list)-1)]\n",
    "    city = city_list[random.randint(0,len(city_list)-1)]\n",
    "    year = random.randint(1990,2025)\n",
    "    month = random.randint(1,12)\n",
    "    day = random.randint(1,28)\n",
    "    time_str = '{}-{}-{}'.format(year,month,day)\n",
    "    date_field = time.strftime(Q[1],time.strptime(time_str,'%Y-%m-%d'))\n",
    "\n",
    "    Q=Q[0].format(city=city,year=year,month=month,day=day)\n",
    "    A='城市:%s\\n日期:%s'%(city,date_field)\n",
    "\n",
    "    example = {\n",
    "        'id':'identity_{}'.format(i),\n",
    "        'conversations':[\n",
    "            {\n",
    "                \"from\": \"user\",\n",
    "                \"value\": prompt_template%(Q,)\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"assistant\",\n",
    "                \"value\": A,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    # print(example)\n",
    "    train_data.append(example)\n",
    "\n",
    "with open('train_data.text','w',encoding='utf-8') as fp:\n",
    "    fp.write(json.dumps(train_data))\n",
    "\n",
    "print(\"样本数量:\",len(train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c8064-8b4d-4c85-9b76-4cff00ed8df5",
   "metadata": {},
   "source": [
    "# 测试微调之后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362f399e-d17a-4d5b-a1bb-d883661ef9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Some weights of the model checkpoint at /home/ubuntu/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.3.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 151851. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    '/home/ubuntu/nn/Qwen/output_qwen', # path to the output directory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3192088-e1c3-4a23-b7d5-ce343710c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.top_p = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a028a0fe-34ba-445a-82ca-211dc3ce8519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:2024年4月15号三亚下雨吗？,\n",
      "A:城市:三亚\n",
      "日期:2024-04-15\n",
      "\n",
      "Q:上海3-16号天气预报,\n",
      "A:城市:上海\n",
      "日期:03-16\n",
      "\n",
      "Q:5月5号下雨吗？城市是苏州,\n",
      "A:城市:苏州\n",
      "日期:05-05\n",
      "\n",
      "Q:杭州2024年3月1日有雾霾,\n",
      "A:城市:杭州\n",
      "日期:2024-03-01\n",
      "\n",
      "Q:我计划6月1号去南京旅游，请问天气怎么样？,\n",
      "A:城市:南京\n",
      "日期:06-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_list = ['2024年4月15号三亚下雨吗？',\n",
    "          '上海3-16号天气预报',\n",
    "          '5月5号下雨吗？城市是苏州',\n",
    "          '杭州2024年3月1日有雾霾',\n",
    "          '我计划6月1号去南京旅游，请问天气怎么样？']\n",
    "\n",
    "for Q in Q_list:\n",
    "    prompt=prompt_template%(Q,)\n",
    "    resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "    print('Q:%s,\\nA:%s\\n'%(Q,resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5a543-8d8a-4df1-aa47-003ef8e109c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "xxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
